{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96d9e0d",
   "metadata": {},
   "source": [
    "# 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d636ae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/aiffel/aiffel/lyricst/data/lyrics/janisjoplin.txt', '/aiffel/aiffel/lyricst/data/lyrics/nursery_rhymes.txt', '/aiffel/aiffel/lyricst/data/lyrics/dickinson.txt', '/aiffel/aiffel/lyricst/data/lyrics/nicki-minaj.txt', '/aiffel/aiffel/lyricst/data/lyrics/disney.txt', '/aiffel/aiffel/lyricst/data/lyrics/bob-marley.txt', '/aiffel/aiffel/lyricst/data/lyrics/beatles.txt', '/aiffel/aiffel/lyricst/data/lyrics/dj-khaled.txt', '/aiffel/aiffel/lyricst/data/lyrics/kanye.txt', '/aiffel/aiffel/lyricst/data/lyrics/patti-smith.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricst/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "print(txt_list[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bda87d",
   "metadata": {},
   "source": [
    "# 데이터 정제 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f94539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Busted flat in Baton Rouge, waitin' for a train\", \"And I's feelin' near as faded as my jeans\", 'Bobby thumbed a diesel down, just before it rained']\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a333e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> busted flat in baton rouge , waitin for a train <end>', '<start> and i s feelin near as faded as my jeans <end>', '<start> bobby thumbed a diesel down , just before it rained <end>']\n"
     ]
    }
   ],
   "source": [
    "#문장을 전처리시키는 함수입니다.\n",
    "def preprosentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "for save in raw_corpus:\n",
    "    x = preprosentence(save)\n",
    "    corpus.append(x)\n",
    "\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b65f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  11 2706  131 ...  456 6490    3]\n",
      " [   2 5633 7665 ...    0    0    0]\n",
      " [   2   23 1731 ...  383    3    0]\n",
      " ...\n",
      " [   2    5   22 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f68941f7df0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus1):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words =12000, filters=' ', oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus1)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus1)\n",
    "    \n",
    "    \n",
    "    tensor_fifteen = []\n",
    "    for tensor_a in tensor:\n",
    "        if len(tensor_a) >= 15:\n",
    "            tensor_fifteen.append(tensor_a)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor_fifteen,padding='post',maxlen=20)\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a08ae8",
   "metadata": {},
   "source": [
    "# 학습을 위한 모델 구조 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b6efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:] \n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161d02fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 19), (128, 19)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e384d8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 19), (128, 19)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_val)\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = len(enc_val) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "dataset1 = dataset1.shuffle(BUFFER_SIZE)\n",
    "dataset1 = dataset1.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be92adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "embedding_size = 1280\n",
    "hidden_size = 1300\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c26460",
   "metadata": {},
   "source": [
    "# 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "238800bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() # Adam은 현재 가장 많이 사용하는 옵티마이저이다. 자세한 내용은 차차 배운다\n",
    "\n",
    "#Loss\n",
    "# tf.keras.losses.SparseCategoricalCrossentropy : https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, reduction='none') # 클래스 분류 문제에서 softmax 함수를 거치면 from_logits = False(default값),그렇지 않으면 from_logits = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21cb113e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "161/161 [==============================] - 44s 222ms/step - loss: 5.2741 - val_loss: 4.8326\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 38s 236ms/step - loss: 4.5402 - val_loss: 4.4244\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 41s 252ms/step - loss: 4.2306 - val_loss: 4.2410\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 41s 255ms/step - loss: 4.0330 - val_loss: 4.1193\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 41s 255ms/step - loss: 3.8633 - val_loss: 4.0164\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 41s 257ms/step - loss: 3.7104 - val_loss: 3.9351\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 41s 257ms/step - loss: 3.5684 - val_loss: 3.8732\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 41s 255ms/step - loss: 3.4284 - val_loss: 3.8151\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 41s 256ms/step - loss: 3.2931 - val_loss: 3.7712\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 41s 256ms/step - loss: 3.1571 - val_loss: 3.7186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6788308bb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정했다.\n",
    "model.fit(dataset, epochs=10, validation_data=dataset1)\n",
    "\n",
    "#Val_Loss값 최소 3.5479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35adf979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis = -1), axis = -1)[:, -1]\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis = -1)\n",
    "        \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len:break\n",
    "            \n",
    "        generated = \"\"\n",
    "        for word_index in test_tensor[0].numpy():\n",
    "            generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45bd8257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e5d456",
   "metadata": {},
   "source": [
    "학습을 돌리는 과정에서 val_loss값이 2.2이하로 낮아지질 않아서 배치,임베딩,은닉사이즈를 늘려보고 줄여봐도\n",
    "2.2이하로 내리진 못했습니다. 그래도 문장을 생성하는것에는 문제가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b3af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
